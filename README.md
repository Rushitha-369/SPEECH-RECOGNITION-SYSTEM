# SPEECH-RECOGNITION-SYSTEM

**COMPANY**: CODTECH IT SOLUTIONS

**NAME**: GUNDRATHI RUSHITHA

**INTERN ID**:CT08DM1042

**DOMAIN**: EMBEDDED SYSTEM

**DURATION**:8 WEEEKS

**MENTOR**: NEELA SANTOSH

# DESCRIPTION OF TASK LIKE HOW YOU PERFORMED AND WHAT DONE AND PASTE PICTURES OF OUTPUT:
I have collected the basic information and gained the knowledge regarding to the task and the task is speech recognition system The backbone of any speech recognition system is a well-organized dataset of spoken words or phrases. Unlike AI systems that use massive pre-collected datasets, a human-created system involves manually collecting audio recordings. Volunteers or participants are recorded speaking the target vocabulary in various tones and environments. This ensures diversity in the dataset. Each recording is carefully labeled with the correct transcription. 
MIT App Inventor is an online tool that allows users to build Android applications through drag-and-drop blocks instead of traditional coding. It is designed to help anyone — especially those with little to no programming experience — create mobile apps. This tool is especially useful for educational purposes, where learners can bring their creative ideas to life.
The goal of the project was to create a simple speech recognition app that listens to a user's voice and converts it into text. The app would then display the recognized speech on the screen or trigger certain actions based on the spoken words (like opening a screen, turning on a light via Bluetooth, or speaking the text back using Text-to-Speech).
The first step in the development process involved creating the user interface (UI) using the Designer tab in App Inventor. The human developer manually placed the following components on the screen:
A Button to start speech recognition.
A Label to display the recognized speech text.
A SpeechRecognizer component (non-visible).
Optionally, a Text-to-Speech component for feedback.
Each component was added intentionally by the developer based on the purpose and design of the app.
The real logic of the app was created in the Blocks Editor section. Here, the developer dragged and connected visual code blocks to define how the app would behave when the user interacts with it. 
After completing the design and logic, the developer used the MIT AI2 Companion app or an emulator to test the app on a real Android device. This allowed them to observe how accurately the app could recognize speech. If there were errors or unexpected results (like misheard words), the developer went back to the blocks section to refine the logic or change the interface.

**OUTPUT**:<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/28ebe0ba-d6e4-4334-8933-2a022ff75ea8" />
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/1de874ae-ccaf-47b0-86ab-32690f02edc5" />
![Image](https://github.com/user-attachments/assets/b1e99393-b3b7-4d72-99dc-ca573ce7b6bc)
![Image](https://github.com/user-attachments/assets/6af41789-ba1a-4a71-acbb-8e130b4b1d66)
![Image](https://github.com/user-attachments/assets/46fdf241-f658-43c5-962f-66f685685f3b)



